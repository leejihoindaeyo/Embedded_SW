{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10270766,"sourceType":"datasetVersion","datasetId":6354703}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef load_data(data_dir, img_size=(128, 128)):\n    images = []\n    masks = []\n\n    for img_file in os.listdir(data_dir):\n        if '_mask' not in img_file:\n            img_path = os.path.join(data_dir, img_file)\n            mask_path = os.path.join(data_dir, img_file.replace('.jpg', '_mask.jpg'))\n    \n            img = load_img(img_path, target_size=img_size)\n            mask = load_img(mask_path, target_size=img_size, color_mode='grayscale')\n    \n            img = img_to_array(img) / 255.0\n            mask = img_to_array(mask) / 255.0\n    \n            images.append(img)\n            masks.append(mask)\n\n    return np.array(images), np.array(masks)\n\ntrain_dir = '/kaggle/input/unet12342/split_datasets/train'\nval_dir = '/kaggle/input/unet12342/split_datasets/val'\ntest_dir = '/kaggle/input/unet12342/split_datasets/test'\n\nX_train, y_train = load_data(train_dir)\nX_val, y_val = load_data(val_dir)\nX_test, y_test = load_data(test_dir)\n\n# EfficientNet의 기본 블록 구현\ndef conv_block(x, filters, kernel_size=3, strides=1, padding='same'):\n    x = layers.Conv2D(filters, kernel_size, strides=strides, padding=padding)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    return x\n\ndef mb_conv_block(x, filters, expansion_factor, strides):\n    # 확장 레이어\n    in_channels = x.shape[-1]\n    x = conv_block(x, in_channels * expansion_factor, kernel_size=1)\n    \n    # Depthwise Convolution\n    x = layers.DepthwiseConv2D(kernel_size=3, strides=strides, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    \n    # Pointwise Convolution\n    x = conv_block(x, filters, kernel_size=1)\n    return x\n\ndef build_efficientnet_unet(input_shape=(128, 128, 3)):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder\n    x = conv_block(inputs, 32)\n    x = mb_conv_block(x, filters=16, expansion_factor=1, strides=1)  # Block 1\n    skip1 = x\n\n    x = mb_conv_block(x, filters=24, expansion_factor=6, strides=2)  # Block 2\n    skip2 = x\n\n    x = mb_conv_block(x, filters=40, expansion_factor=6, strides=2)  # Block 3\n    skip3 = x\n\n    x = mb_conv_block(x, filters=80, expansion_factor=6, strides=2)  # Block 4\n    skip4 = x\n\n    x = mb_conv_block(x, filters=112, expansion_factor=6, strides=1)  # Block 5\n    x = mb_conv_block(x, filters=192, expansion_factor=6, strides=2)  # Block 6\n\n    # Bottleneck\n    x = mb_conv_block(x, filters=320, expansion_factor=6, strides=1)  # Block 7\n\n    # Decoder\n    x = layers.Conv2DTranspose(192, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.concatenate([x, skip4])\n    x = conv_block(x, 192)\n\n    x = layers.Conv2DTranspose(112, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.concatenate([x, skip3])\n    x = conv_block(x, 112)\n\n    x = layers.Conv2DTranspose(80, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.concatenate([x, skip2])\n    x = conv_block(x, 80)\n\n    x = layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(x)\n    x = layers.concatenate([x, skip1])\n    x = conv_block(x, 32)\n\n    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(x)\n\n    model = models.Model(inputs=inputs, outputs=outputs)\n    return model\n\nmodel = build_efficientnet_unet()\nmodel.summary()\n\ndef dice_coef(y_true, y_pred, smooth=1e-6):\n    y_pred = tf.cast(y_pred > 0.5, dtype=tf.float32)\n    intersection = tf.reduce_sum(y_true * y_pred)\n    return (2.0 * intersection + smooth) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + smooth)\n\ndef dice_loss(y_true, y_pred):\n    return 1 - dice_coef(y_true, y_pred)\n\ndef combined_loss(y_true, y_pred, a=0.95):\n    logit_loss_value = tf.keras.losses.binary_crossentropy(y_true, y_pred, from_logits=False)\n    dice_loss_value = dice_loss(y_true, y_pred)\n    return a * dice_loss_value + (1 - a) * logit_loss_value\n\nmodel.compile(optimizer='adam', loss=combined_loss, metrics=[dice_coef])\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_dice_coef',\n    factor=0.9,\n    patience=3,\n    min_lr=1e-6,\n    mode='max',\n    verbose=1\n)\n\nmodel_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    'best_efficientnet_unet_model.keras',\n    monitor='val_dice_coef',\n    save_best_only=True,\n    mode='max',\n    verbose=1\n)\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=16,\n                    epochs=50,\n                    validation_data=(X_val, y_val),\n                    callbacks=[reduce_lr, model_checkpoint]\n                   )\n\nmodel.save('efficientnet_unet_model.keras')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_model = tf.keras.models.load_model('best_unet_model.keras', \n                                        custom_objects={'combined_loss': combined_loss,\n                                                        'dice_coef': dice_coef})\n\ntest_loss, test_dice = best_model.evaluate(X_test, y_test)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.subplot(1, 2, 2)\nplt.plot(history.history['dice_coef'])\nplt.plot(history.history['val_dice_coef'])\nplt.title('Model dice_coef')\nplt.ylabel('dice_coef')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='lower right')\nplt.tight_layout()\nplt.show()\n\ndef visualize_results(model, X, y, num_samples=3):\n    predictions = model.predict(X[:num_samples])\n    predictions = (predictions > 0.5).astype(np.float32)\n    plt.figure(figsize=(4*num_samples, 12))\n    for i in range(num_samples):\n        plt.subplot(3, num_samples, i + 1)\n        plt.imshow(X[i])\n        plt.title('Original Image')\n        plt.axis('off')\n        plt.subplot(3, num_samples, i + 1 + num_samples)\n        plt.imshow(y[i, :, :, 0], cmap='gray')\n        plt.title(f'True Mask\\nDice: {dice_coef(y[i:i+1], predictions[i:i+1]).numpy():.4f}')\n        plt.axis('off')\n        plt.subplot(3, num_samples, i + 1 + 2*num_samples)\n        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n        plt.title('Predicted Mask')\n        plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nvisualize_results(model, X_test, y_test)\n\ndef measure_model_size_and_time(model, sample_input_shape):\n    # 모델 사이즈 측정\n    model_size = model.count_params()\n    model_size_bytes = model_size * 4  # 각 파라미터가 4바이트(32비트)라고 가정\n\n    # 계산 시간 측정\n    sample_input = np.random.rand(*sample_input_shape).astype(np.float32)\n\n    start_time = time.time()\n    model.predict(sample_input)\n    end_time = time.time()\n\n    inference_time = end_time - start_time\n\n    print(f\"모델의 파라미터 수: {model_size} (약 {model_size_bytes / (1024 ** 2):.2f} MB)\")\n    print(f\"예측 시간: {inference_time:.4f} 초\")\n\n# 사용 예시\nmeasure_model_size_and_time(model, (1, 128, 128, 3))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}